{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3517de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def get_supported_dtype(device: str) -> torch.dtype:\n",
    "    if device == \"cuda\" and torch.cuda.is_bf16_supported():\n",
    "        return torch.bfloat16\n",
    "    elif device == \"cuda\" and torch.cuda.is_fp16_supported():\n",
    "        return torch.float16\n",
    "    else:\n",
    "        return torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9d598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 3 bullet points summarizing the role of LoRA in AI:\n",
      "\n",
      "*   LoRA (Low-Rank Adaptation) allows you to fine-tune an AI model without needing to train it from scratch, making it more efficient and adaptable to new tasks or datasets.\n",
      "*   It leverages pre-trained models and fine-tuning techniques to achieve state-of-the-art performance on specific tasks or datasets.\n"
     ]
    }
   ],
   "source": [
    "# pip install -U \"transformers>=4.41\" \"peft>=0.11\" \"accelerate>=0.33\" torch\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# ---- config ----\n",
    "BASE_MODEL_ID = \"google/gemma-3-270m-it\"          # change me\n",
    "LORA_ADAPTER_ID = \"boreasg/sft-model-runpod-test_20260123_065252\"     # change me\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE = get_supported_dtype(DEVICE)\n",
    "\n",
    "# ---- load tokenizer ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, use_fast=True)\n",
    "\n",
    "# ---- load two base instances so we can compare the raw base model vs the SFT (PEFT) model ----\n",
    "device_map = \"auto\" if DEVICE == \"cuda\" else None\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=device_map,\n",
    ")\n",
    "# load a second copy to apply the adapter onto (keep base_model untouched)\n",
    "base_for_peft = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=device_map,\n",
    ")\n",
    "sft_model = PeftModel.from_pretrained(base_for_peft, LORA_ADAPTER_ID)\n",
    "\n",
    "# eval mode\n",
    "base_model.eval()\n",
    "sft_model.eval()\n",
    "\n",
    "# quick debug prints\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"base_model first param dtype:\", next(base_model.parameters()).dtype)\n",
    "print(\"sft_model first param dtype:\", next(sft_model.parameters()).dtype)\n",
    "print(\"tokenizer eos_token_id:\", tokenizer.eos_token_id)\n",
    "\n",
    "# ---- generation helper that can run any model instance ----\n",
    "@torch.inference_mode()\n",
    "def generate_with_model(model, prompt: str, *, max_new_tokens: int = 512, do_sample: bool = False) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful, concise assistant. Answer directly and accurately.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "\n",
    "    if hasattr(tokenizer, \"apply_chat_template\") and tokenizer.chat_template:\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    else:\n",
    "        text = f\"System: You are a helpful, concise assistant. Answer directly and accurately.\\nUser: {prompt}\\nAssistant:\"\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # send inputs to the device where the model lives (works with device_map placements)\n",
    "    model_device = next(model.parameters()).device\n",
    "    inputs = {k: v.to(model_device) for k, v in inputs.items()}\n",
    "\n",
    "    # sanity-check: single forward to inspect logits\n",
    "    try:\n",
    "        logits = model(**inputs).logits\n",
    "        print(f\"Sanity: logits nan? {logits.isnan().any().item()} max_abs={logits.abs().max().item():.3e}\")\n",
    "    except Exception as e:\n",
    "        print(\"Sanity forward failed:\", e)\n",
    "\n",
    "    out = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=do_sample,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    output_text = tokenizer.decode(out[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return output_text.strip()\n",
    "\n",
    "# ---- compare runner ----\n",
    "def compare_models(prompt: str):\n",
    "    print(\"--- Prompt ---\")\n",
    "    print(prompt)\n",
    "    print()\n",
    "\n",
    "    print(\"--- Base model output ---\")\n",
    "    try:\n",
    "        base_out = generate_with_model(base_model, prompt)\n",
    "        print(base_out)\n",
    "    except Exception as e:\n",
    "        print(\"Base generation failed:\", e)\n",
    "\n",
    "    print()\n",
    "    print(\"--- SFT (PEFT) model output ---\")\n",
    "    try:\n",
    "        sft_out = generate_with_model(sft_model, prompt)\n",
    "        print(sft_out)\n",
    "    except Exception as e:\n",
    "        print(\"SFT generation failed:\", e)\n",
    "\n",
    "    return {\"base\": base_out if 'base_out' in locals() else None, \"sft\": sft_out if 'sft_out' in locals() else None}\n",
    "\n",
    "# ---- example ----\n",
    "if __name__ == \"__main__\":\n",
    "    result = compare_models(\"Explain LoRA in 3 bullet points.\")\n",
    "    # result contains both outputs for programmatic use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85bb9926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi [Friend's Name], Just wanted to say hi! Hope you're doing well. Let me know if you need anything!\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"Write a short message to a friend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81380f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collablllm-repro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
