{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933ade05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "793c197e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8c0d26b1-2345-4fc6-834f-20da2853a30b)')' thrown while requesting HEAD https://huggingface.co/google/gemma-3-270m/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipeline = pipeline(task=\"text-generation\", model=\"google/gemma-3-270m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753c8f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'who are you?If you\\'ve been following me on social media, you probably know that I am a big fan of the <b><i>Marvel</i></b> comics.  I love the characters and I love the plots.  I just love the world of comics.  I also love the idea of comics being so different from the \"other\" comic, but at the same time, I love the idea of it being the \"other\" comic.  I love the idea that comics are a medium that we use to expand our knowledge of the world, and that we can share that knowledge.\\n\\nI recently came across a story that I liked.  I was going to tell it to a friend, but I didn\\'t know how.  I\\'m not a comic book person, but I think that I would be interested in reading this story, so I read it.  I read it in a couple of days, but I had no idea what to expect.  I was just really intrigued by the story.\\n\\nI have a really long story to tell.  It\\'s about my dad, who was in prison and was trying to figure out how he was going to pay for the cost of his freedom.  I was in a lot of trouble, and he'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "956aeb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 21:30:50,393 - urllib3.connectionpool - DEBUG - Resetting dropped connection: huggingface.co\n",
      "2026-01-21 21:30:50,604 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /datasets/collabllm/collabllm-multiturn-medium-large/resolve/main/README.md HTTP/1.1\" 307 0\n",
      "2026-01-21 21:30:50,648 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /api/resolve-cache/datasets/collabllm/collabllm-multiturn-medium-large/b921e19eac426e5c7788c38ebea9ddb760999d0c/README.md HTTP/1.1\" 200 0\n",
      "2026-01-21 21:30:50,764 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /datasets/collabllm/collabllm-multiturn-medium-large/resolve/b921e19eac426e5c7788c38ebea9ddb760999d0c/collabllm-multiturn-medium-large.py HTTP/1.1\" 404 0\n",
      "2026-01-21 21:30:50,768 - urllib3.connectionpool - DEBUG - Resetting dropped connection: s3.amazonaws.com\n",
      "2026-01-21 21:30:51,072 - urllib3.connectionpool - DEBUG - https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/collabllm/collabllm-multiturn-medium-large/collabllm/collabllm-multiturn-medium-large.py HTTP/1.1\" 404 0\n",
      "2026-01-21 21:30:51,211 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /datasets/collabllm/collabllm-multiturn-medium-large/resolve/b921e19eac426e5c7788c38ebea9ddb760999d0c/.huggingface.yaml HTTP/1.1\" 404 0\n",
      "2026-01-21 21:30:51,219 - urllib3.connectionpool - DEBUG - Resetting dropped connection: datasets-server.huggingface.co\n",
      "2026-01-21 21:30:51,433 - urllib3.connectionpool - DEBUG - https://datasets-server.huggingface.co:443 \"GET /info?dataset=collabllm/collabllm-multiturn-medium-large HTTP/1.1\" 200 None\n",
      "2026-01-21 21:30:51,564 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /datasets/collabllm/collabllm-multiturn-medium-large/resolve/b921e19eac426e5c7788c38ebea9ddb760999d0c/dataset_infos.json HTTP/1.1\" 404 0\n",
      "2026-01-21 21:30:51,570 - filelock - DEBUG - Attempting to acquire lock 15266347808 on /Users/robertgordan/.cache/huggingface/datasets/_Users_robertgordan_.cache_huggingface_datasets_collabllm___collabllm-multiturn-medium-large_default_0.0.0_b921e19eac426e5c7788c38ebea9ddb760999d0c.lock\n",
      "2026-01-21 21:30:51,571 - filelock - DEBUG - Lock 15266347808 acquired on /Users/robertgordan/.cache/huggingface/datasets/_Users_robertgordan_.cache_huggingface_datasets_collabllm___collabllm-multiturn-medium-large_default_0.0.0_b921e19eac426e5c7788c38ebea9ddb760999d0c.lock\n",
      "2026-01-21 21:30:51,573 - fsspec.local - DEBUG - open file: /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c/dataset_info.json\n",
      "2026-01-21 21:30:51,575 - filelock - DEBUG - Attempting to release lock 15266347808 on /Users/robertgordan/.cache/huggingface/datasets/_Users_robertgordan_.cache_huggingface_datasets_collabllm___collabllm-multiturn-medium-large_default_0.0.0_b921e19eac426e5c7788c38ebea9ddb760999d0c.lock\n",
      "2026-01-21 21:30:51,576 - filelock - DEBUG - Lock 15266347808 released on /Users/robertgordan/.cache/huggingface/datasets/_Users_robertgordan_.cache_huggingface_datasets_collabllm___collabllm-multiturn-medium-large_default_0.0.0_b921e19eac426e5c7788c38ebea9ddb760999d0c.lock\n",
      "2026-01-21 21:30:51,579 - filelock - DEBUG - Attempting to acquire lock 5507052736 on /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c_builder.lock\n",
      "2026-01-21 21:30:51,580 - filelock - DEBUG - Lock 5507052736 acquired on /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c_builder.lock\n",
      "2026-01-21 21:30:51,581 - fsspec.local - DEBUG - open file: /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c/dataset_info.json\n",
      "2026-01-21 21:30:51,583 - filelock - DEBUG - Attempting to release lock 5507052736 on /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c_builder.lock\n",
      "2026-01-21 21:30:51,584 - filelock - DEBUG - Lock 5507052736 released on /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c_builder.lock\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"collabllm/collabllm-multiturn-medium-large\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f043fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conv_id', 'prompt', 'completion', 'score', 'turn_id', 'single_turn_prompt', 'single_turn_completion', 'single_turn_metadata', 'sessions', 'rewards'],\n",
       "    num_rows: 4606\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7b33bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import multiturn_dataset_to_sft\n",
    "dataset_clean = multiturn_dataset_to_sft(dataset, eval_ratio=0.1, lower_bound_metric=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0058d9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 447\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 49\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e01e9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 21:39:04,523 - __main__ - DEBUG - Now this will appear!\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Basic configuration - logs to console\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,  # Show all levels\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.debug(\"Now this will appear!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a02b5c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-21 21:44:52,492 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /datasets/collabllm/collabllm-multiturn-medium-large/resolve/main/README.md HTTP/1.1\" 307 0\n",
      "2026-01-21 21:44:52,534 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /api/resolve-cache/datasets/collabllm/collabllm-multiturn-medium-large/b921e19eac426e5c7788c38ebea9ddb760999d0c/README.md HTTP/1.1\" 200 0\n",
      "2026-01-21 21:44:52,658 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /datasets/collabllm/collabllm-multiturn-medium-large/resolve/b921e19eac426e5c7788c38ebea9ddb760999d0c/collabllm-multiturn-medium-large.py HTTP/1.1\" 404 0\n",
      "2026-01-21 21:44:52,664 - urllib3.connectionpool - DEBUG - Resetting dropped connection: s3.amazonaws.com\n",
      "2026-01-21 21:44:52,961 - urllib3.connectionpool - DEBUG - https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/collabllm/collabllm-multiturn-medium-large/collabllm/collabllm-multiturn-medium-large.py HTTP/1.1\" 404 0\n",
      "2026-01-21 21:44:53,115 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /datasets/collabllm/collabllm-multiturn-medium-large/resolve/b921e19eac426e5c7788c38ebea9ddb760999d0c/.huggingface.yaml HTTP/1.1\" 404 0\n",
      "2026-01-21 21:44:53,124 - urllib3.connectionpool - DEBUG - Resetting dropped connection: datasets-server.huggingface.co\n",
      "2026-01-21 21:44:53,349 - urllib3.connectionpool - DEBUG - https://datasets-server.huggingface.co:443 \"GET /info?dataset=collabllm/collabllm-multiturn-medium-large HTTP/1.1\" 200 None\n",
      "2026-01-21 21:44:53,471 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /datasets/collabllm/collabllm-multiturn-medium-large/resolve/b921e19eac426e5c7788c38ebea9ddb760999d0c/dataset_infos.json HTTP/1.1\" 404 0\n",
      "2026-01-21 21:44:53,477 - filelock - DEBUG - Attempting to acquire lock 15425132704 on /Users/robertgordan/.cache/huggingface/datasets/_Users_robertgordan_.cache_huggingface_datasets_collabllm___collabllm-multiturn-medium-large_default_0.0.0_b921e19eac426e5c7788c38ebea9ddb760999d0c.lock\n",
      "2026-01-21 21:44:53,479 - filelock - DEBUG - Lock 15425132704 acquired on /Users/robertgordan/.cache/huggingface/datasets/_Users_robertgordan_.cache_huggingface_datasets_collabllm___collabllm-multiturn-medium-large_default_0.0.0_b921e19eac426e5c7788c38ebea9ddb760999d0c.lock\n",
      "2026-01-21 21:44:53,480 - fsspec.local - DEBUG - open file: /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c/dataset_info.json\n",
      "2026-01-21 21:44:53,484 - filelock - DEBUG - Attempting to release lock 15425132704 on /Users/robertgordan/.cache/huggingface/datasets/_Users_robertgordan_.cache_huggingface_datasets_collabllm___collabllm-multiturn-medium-large_default_0.0.0_b921e19eac426e5c7788c38ebea9ddb760999d0c.lock\n",
      "2026-01-21 21:44:53,485 - filelock - DEBUG - Lock 15425132704 released on /Users/robertgordan/.cache/huggingface/datasets/_Users_robertgordan_.cache_huggingface_datasets_collabllm___collabllm-multiturn-medium-large_default_0.0.0_b921e19eac426e5c7788c38ebea9ddb760999d0c.lock\n",
      "2026-01-21 21:44:53,489 - filelock - DEBUG - Attempting to acquire lock 15425132704 on /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c_builder.lock\n",
      "2026-01-21 21:44:53,489 - filelock - DEBUG - Lock 15425132704 acquired on /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c_builder.lock\n",
      "2026-01-21 21:44:53,490 - fsspec.local - DEBUG - open file: /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c/dataset_info.json\n",
      "2026-01-21 21:44:53,492 - filelock - DEBUG - Attempting to release lock 15425132704 on /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c_builder.lock\n",
      "2026-01-21 21:44:53,493 - filelock - DEBUG - Lock 15425132704 released on /Users/robertgordan/.cache/huggingface/datasets/collabllm___collabllm-multiturn-medium-large/default/0.0.0/b921e19eac426e5c7788c38ebea9ddb760999d0c_builder.lock\n",
      "2026-01-21 21:44:54,311 - train_sft - INFO - Dataset collabllm/collabllm-multiturn-medium-large loaded.\n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conv_id', 'prompt', 'completion', 'score', 'turn_id', 'single_turn_prompt', 'single_turn_completion', 'single_turn_metadata', 'sessions', 'rewards'],\n",
      "        num_rows: 4606\n",
      "    })\n",
      "})\n",
      "2026-01-21 21:46:05,986 - dataset_utils - ERROR - Failed processing dataset row: train with error: string indices must be integers\n",
      "2026-01-21 21:46:05,996 - train_sft - INFO - Dataset collabllm/collabllm-multiturn-medium-large preprocessed.\n",
      " DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: [],\n",
      "        num_rows: 0\n",
      "    })\n",
      "    eval: Dataset({\n",
      "        features: [],\n",
      "        num_rows: 0\n",
      "    })\n",
      "})\n",
      "2026-01-21 21:46:06,210 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /google/gemma-3-270m-it/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2026-01-21 21:46:09,338 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /google/gemma-3-270m-it/resolve/main/generation_config.json HTTP/1.1\" 200 0\n",
      "2026-01-21 21:46:09,462 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /google/gemma-3-270m-it/resolve/main/custom_generate/generate.py HTTP/1.1\" 404 0\n",
      "2026-01-21 21:46:09,615 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"HEAD /google/gemma-3-270m-it/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "2026-01-21 21:46:09,852 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/google/gemma-3-270m-it/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "2026-01-21 21:46:09,964 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/google/gemma-3-270m-it/tree/main?recursive=True&expand=False HTTP/1.1\" 200 1728\n",
      "2026-01-21 21:46:10,664 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 \"GET /api/models/google/gemma-3-270m-it HTTP/1.1\" 200 7125\n",
      "2026-01-21 21:46:10,671 - train_sft - INFO - Model google/gemma-3-270m-it loaded with memory footprint: 0.36 GB\n",
      "2026-01-21 21:46:10,837 - trl.trainer.sft_trainer - WARNING - Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn2, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrain_sft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_and_train_sft\n\u001b[0;32m----> 2\u001b[0m \u001b[43mload_and_train_sft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgoogle/gemma-3-270m-it\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_dataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollabllm/collabllm-multiturn-medium-large\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/train_sft.py:89\u001b[0m, in \u001b[0;36mload_and_train_sft\u001b[0;34m(hf_model_path, hf_dataset_path, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m     79\u001b[0m training_arguments \u001b[38;5;241m=\u001b[39m SFTConfig(\n\u001b[1;32m     80\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     81\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     packing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     87\u001b[0m )\n\u001b[0;32m---> 89\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_clean\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_arguments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:783\u001b[0m, in \u001b[0;36mSFTTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[0m\n\u001b[1;32m    775\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    776\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using a per_device_train_batch_size of 1 with padding-free training. Using a batch size \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    777\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof 1 annihilate the benefits of padding-free training. Please consider increasing the batch size \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    778\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto at least 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    779\u001b[0m         )\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# Decide whether to use completion-only loss: if not specified, then it is set to True if the dataset format\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# is prompt-completion, and False if the dataset format is language modeling.\u001b[39;00m\n\u001b[0;32m--> 783\u001b[0m dataset_sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcompletion_only_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_only_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset_sample \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompletion\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset_sample\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from train_sft import load_and_train_sft\n",
    "load_and_train_sft(\n",
    "    hf_model_path=\"google/gemma-3-270m-it\",\n",
    "    hf_dataset_path=\"collabllm/collabllm-multiturn-medium-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8b34ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf926983bd284950b1f962c08d0685a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': [{'content': 'Hey, can you help me create a promotional document for our custom brand jewelry and accessories? I want it to focus on the quality of our products and how popular they are globally.', 'role': 'user'}], 'completion': [{'content': \"To create an effective promotional document for your custom brand jewelry and accessories, it's important to consider a few details. Could you let me know the format you have in mind? For example, are you thinking of a brochure, a digital flyer, or a more detailed catalog? Additionally, who is your target audience for this document? Knowing whether it's for retail partners, end consumers, or a different group will help tailor the content. \\n\\nAs for content, would you like to include specific product highlights, customer testimonials, or any data about your global popularity? Let me know what you envision, and I can help draft a structure or even some sample content for you. Feel free to share any additional preferences or ideas you have!\", 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(example):\n",
    "    return {\n",
    "        \"prompt\": example[\"prompt\"],\n",
    "        \"completion\": [{\"role\" : \"assistant\", \"content\": example[\"completion\"]}]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(preprocess_function, remove_columns=dataset.column_names)\n",
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22c11eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa33a3310e946928afefb14fae23a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/520 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fdf2c3e8de4582a425daf95e23659a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/37.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82825a8527134f20aa2c3c024e288637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/482k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bb46856baff4e1db8849ec3b2efed12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/15806 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3316d7ac5ce449e9bba93312eadf227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f25402abcd8849de8588d91f384c4988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/15806 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35c8258abb647829fcd198569e8de2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/15806 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "/Users/robertgordan/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 15.84 GiB, other allocations: 513.94 MiB, max allowed: 18.13 GiB). Tried to allocate 4.64 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      4\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SFTTrainer(\n\u001b[1;32m      5\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQwen/Qwen3-0.6B\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mload_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrl-lib/Capybara\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:1185\u001b[0m, in \u001b[0;36mSFTTrainer.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[0;32m-> 1185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/transformers/trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   4026\u001b[0m ):\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:1098\u001b[0m, in \u001b[0;36mSFTTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;66;03m# If not set, defaults from model config and may warn since cache isn't compatible with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m (loss, outputs) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;66;03m# Compute entropy\u001b[39;00m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_liger_kernel:  \u001b[38;5;66;03m# liger doesn't return logits\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/transformers/trainer.py:4110\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   4108\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   4109\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m-> 4110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   4112\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   4113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/transformers/models/qwen3/modeling_qwen3.py:498\u001b[0m, in \u001b[0;36mQwen3ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 498\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CausalLMOutputWithPast(\n\u001b[1;32m    501\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m    502\u001b[0m     logits\u001b[38;5;241m=\u001b[39mlogits,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    505\u001b[0m     attentions\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mattentions,\n\u001b[1;32m    506\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/transformers/loss/loss_utils.py:67\u001b[0m, in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Enable model parallelism\u001b[39;00m\n\u001b[1;32m     66\u001b[0m shift_labels \u001b[38;5;241m=\u001b[39m shift_labels\u001b[38;5;241m.\u001b[39mto(logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 67\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mfixed_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/transformers/loss/loss_utils.py:36\u001b[0m, in \u001b[0;36mfixed_cross_entropy\u001b[0;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfixed_cross_entropy\u001b[39m(\n\u001b[1;32m     29\u001b[0m     source: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     30\u001b[0m     target: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     34\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     35\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 36\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# just in case users pass an int for num_items_in_batch, which could be the case for custom trainer\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_tensor(num_items_in_batch):\n",
      "File \u001b[0;32m~/Projects/CollablLLM-repro/.venv/lib/python3.9/site-packages/torch/nn/functional.py:3462\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3461\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 15.84 GiB, other allocations: 513.94 MiB, max allowed: 18.13 GiB). Tried to allocate 4.64 GiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from datasets import load_dataset\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=\"Qwen/Qwen3-0.6B\",\n",
    "    train_dataset=load_dataset(\"trl-lib/Capybara\", split=\"train\"))\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf717c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collablllm-repro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
